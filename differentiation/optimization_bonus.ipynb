{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A statistics example\n",
    "\n",
    "Consider we run an experiment and it gives some data that is of the form $$\n",
    "y = m x\n",
    "$$. That's we measure $x$ and $y$, and we know there is some linear relationship between $x$ and $y$, but we don't know the value of $m$.\n",
    "\n",
    "We are given some data points, but those data points are noisy. They don't fit perfectly on the line.\n",
    "\n",
    "![](noisy_linear_data.png)\n",
    "\n",
    "To determine the appropriate value of $m$, we can transform the original problem into an optimization problem.\n",
    "\n",
    "Let's introduce a method called **Least Squares**. Let $S(m)$ be the *sum of squares of distance* between $y_i$ and $m x_i$ for a given $m$, then $$\n",
    "S(m) = \\sum_{i} (y_i - m x_i)^2\n",
    "$$. Then we need to find the $m$ that minimize $S(m)$.\n",
    "\n",
    "By computing the derivative of $S$ respect to $m$, $$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\frac{d S}{d m} &= \\sum_{i} - 2 x_i (y_i - m x_i) \\\\\n",
    "&= -2 \\sum_{i} x_i y_i + 2 m \\sum_{i} x_i^2\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$. The next step is to find the critical points, where\n",
    "$$\n",
    "-2 \\sum_{i} x_i y_i + 2 m \\sum_{i} x_i^2 = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "2 \\sum_{i} x_i y_i = 2 m \\sum_{i} x_i^2 \n",
    "$$\n",
    "\n",
    "$$\n",
    "m = \\frac{\\sum_{i} x_i y_i}{\\sum_{i} x_i^2}\n",
    "$$ So is that a local minimum or a local maximum? Let's compute the $2{nd}$ order derivative. $$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\frac{d^2 S}{d m^2} &= 2 \\sum_{i} x_i^2 > 0\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$. Hence the critical point is the minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A more general form of Linear Regression: $y = m x + b$\n",
    "\n",
    "To extend the application of the least squares method from linear relationship to affine relationship $y = m x + b$, we can form the problem as finding the minimum for $$\n",
    "S(m, b) = \\sum_{i} (y_i - (m x_i + b))^2\n",
    "$$. And this is very interesting because we don't know yet about finding the derivative of a function with more than one input.\n",
    "\n",
    "We will figure multivariable stuff out in a different course. With spoilers, we know that will be useful for a bunch of applications optimizing mulrivariable functions:\n",
    "- Game Theory\n",
    "- Linear Programming\n",
    "- Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
